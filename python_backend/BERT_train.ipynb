{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e55390d9-b173-4431-be5d-cae2bec83f49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Labels: ['cooking' 'fashion' 'fitness' 'programming']\n",
      "Number of Labels: 4\n",
      "Training samples: 316\n",
      "Validation samples: 79\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Load your dataset\n",
    "df = pd.read_csv(\"reddit_bookmark_data.csv\")\n",
    "\n",
    "# Ensure 'title' and 'label' columns exist\n",
    "if 'title' not in df.columns or 'label' not in df.columns:\n",
    "    raise ValueError(\"CSV must contain 'title' and 'label' columns.\")\n",
    "\n",
    "# Handle potential missing titles (though PRAW usually returns them)\n",
    "df.dropna(subset=['title'], inplace=True)\n",
    "\n",
    "# Encode labels to numerical IDs\n",
    "label_encoder = LabelEncoder()\n",
    "df['encoded_label'] = label_encoder.fit_transform(df['label'])\n",
    "\n",
    "# Get the mapping from numerical ID back to original label\n",
    "id_to_label = {id: label for id, label in enumerate(label_encoder.classes_)}\n",
    "num_labels = len(label_encoder.classes_)\n",
    "\n",
    "print(f\"Original Labels: {label_encoder.classes_}\")\n",
    "print(f\"Number of Labels: {num_labels}\")\n",
    "\n",
    "# Split data into training and validation sets\n",
    "# A validation set is crucial for monitoring performance and preventing overfitting.\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "    df['title'].tolist(),\n",
    "    df['encoded_label'].tolist(),\n",
    "    test_size=0.2, # 20% for validation\n",
    "    random_state=42,\n",
    "    stratify=df['encoded_label'] # Stratify to ensure equal distribution of labels in splits\n",
    ")\n",
    "\n",
    "print(f\"Training samples: {len(train_texts)}\")\n",
    "print(f\"Validation samples: {len(val_texts)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9588141f-6b77-4f99-8d9f-6714654c607e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79d6130b56e248e1b0d2cd1818e4ddec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "740c1fecec5a4d78bf094c498915f5aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/684 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c553a050966144578792d36d7e268571",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/760k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d253b6d425e4860ba93004bad6bbf1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.31M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "638ebf7c2e5c48de8a7c8a75ecd93e4a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/47.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at albert/albert-base-v1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "# Choose your pre-trained model checkpoint\n",
    "# 'distilbert-base-uncased' is recommended for a personal project due to size/speed\n",
    "model_name = \"albert/albert-base-v1\"\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Load model for sequence classification\n",
    "# We specify the number of labels and the mapping for better logging/understanding\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=num_labels,\n",
    "    id2label=id_to_label,\n",
    "    label2id={label: id for id, label in id_to_label.items()}\n",
    ")\n",
    "\n",
    "# Move model to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c9d392f1-232b-4181-bf77-79f65d13cc9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_data(texts, labels):\n",
    "    encodings = tokenizer(\n",
    "        texts,\n",
    "        truncation=True,\n",
    "        padding=True,\n",
    "        max_length=tokenizer.model_max_length, # Often 512 for BERT/DistilBERT\n",
    "        return_tensors=\"pt\" # Return PyTorch tensors\n",
    "    )\n",
    "    return {\n",
    "        'input_ids': encodings['input_ids'],\n",
    "        'attention_mask': encodings['attention_mask'],\n",
    "        'labels': torch.tensor(labels)\n",
    "    }\n",
    "\n",
    "train_encodings = tokenize_data(train_texts, train_labels)\n",
    "val_encodings = tokenize_data(val_texts, val_labels)\n",
    "\n",
    "# Create PyTorch Datasets\n",
    "class BookmarkDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings):\n",
    "        self.encodings = encodings\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {key: val[idx] for key, val in self.encodings.items()}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encodings['input_ids'])\n",
    "\n",
    "train_dataset = BookmarkDataset(train_encodings)\n",
    "val_dataset = BookmarkDataset(val_encodings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dea71a1e-cccd-4801-a92f-3559baf00cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import numpy as np\n",
    "\n",
    "# Define compute_metrics function for evaluation during training\n",
    "def compute_metrics(p):\n",
    "    predictions = np.argmax(p.predictions, axis=1)\n",
    "    return {\n",
    "        'accuracy': accuracy_score(p.label_ids, predictions),\n",
    "        'f1_weighted': f1_score(p.label_ids, predictions, average='weighted')\n",
    "    }\n",
    "\n",
    "# Define training arguments\n",
    "# These parameters can be tuned.\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\", # Directory for checkpoints and predictions\n",
    "    num_train_epochs=5, # Number of epochs, start with 3-5\n",
    "    per_device_train_batch_size=16, # Batch size for training\n",
    "    per_device_eval_batch_size=64, # Batch size for evaluation\n",
    "    warmup_steps=500, # Number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.01, # Strength of weight decay\n",
    "    logging_dir=\"./logs\", # Directory for logs\n",
    "    logging_steps=10, # Log training progress every N steps\n",
    "    eval_strategy=\"epoch\", # Evaluate every epoch\n",
    "    save_strategy=\"epoch\", # Save model checkpoint every epoch\n",
    "    load_best_model_at_end=True, # Load the best model at the end of training\n",
    "    metric_for_best_model=\"f1_weighted\", # Metric to use for early stopping/best model selection\n",
    "    greater_is_better=True,\n",
    "    report_to=\"none\", # You can set this to \"wandb\" or \"tensorboard\" for better tracking\n",
    ")\n",
    "\n",
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5f08be52-6f86-4018-bcbd-829ece7a1d58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 04:32, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Weighted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.453300</td>\n",
       "      <td>1.481894</td>\n",
       "      <td>0.253165</td>\n",
       "      <td>0.102289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.332700</td>\n",
       "      <td>1.305997</td>\n",
       "      <td>0.367089</td>\n",
       "      <td>0.273172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.227400</td>\n",
       "      <td>1.150341</td>\n",
       "      <td>0.531646</td>\n",
       "      <td>0.470285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.058500</td>\n",
       "      <td>1.011547</td>\n",
       "      <td>0.620253</td>\n",
       "      <td>0.608734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.881600</td>\n",
       "      <td>0.882638</td>\n",
       "      <td>0.708861</td>\n",
       "      <td>0.705372</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training complete!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2' max='2' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2/2 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Results: {'eval_loss': 0.8826382756233215, 'eval_accuracy': 0.7088607594936709, 'eval_f1_weighted': 0.7053716080543725, 'eval_runtime': 2.942, 'eval_samples_per_second': 26.853, 'eval_steps_per_second': 0.68, 'epoch': 5.0}\n"
     ]
    }
   ],
   "source": [
    "print(\"Starting training...\")\n",
    "trainer.train()\n",
    "print(\"Training complete!\")\n",
    "\n",
    "# Evaluate on the validation set after training\n",
    "results = trainer.evaluate()\n",
    "print(\"Validation Results:\", results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "235b32f4-7184-4e5d-bfcd-d2092cd13afe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and tokenizer saved to ./fine_tuned_distilbert_bookmark_classifier3\n",
      "Label encoder saved to ./fine_tuned_distilbert_bookmark_classifier3/label_encoder.pkl\n"
     ]
    }
   ],
   "source": [
    "# Save the model and tokenizer\n",
    "output_model_dir = \"./fine_tuned_distilbert_bookmark_classifier3\"\n",
    "trainer.save_model(output_model_dir)\n",
    "tokenizer.save_pretrained(output_model_dir)\n",
    "\n",
    "print(f\"Model and tokenizer saved to {output_model_dir}\")\n",
    "\n",
    "# Save the label encoder as well, so you can map predictions back to text labels\n",
    "import joblib\n",
    "joblib.dump(label_encoder, f\"{output_model_dir}/label_encoder.pkl\")\n",
    "print(f\"Label encoder saved to {output_model_dir}/label_encoder.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5162d1e0-fa90-480b-93a2-ace831c82606",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Making predictions on new text ---\n",
      "Text: 'Delicious recipe for carbonara pasta'\n",
      "  Predicted Label: cooking (Confidence: 0.8512)\n",
      "Text: 'Best tips for learning Python in 2024'\n",
      "  Predicted Label: cooking (Confidence: 0.4817)\n",
      "Text: 'Stylish autumn outfits for women'\n",
      "  Predicted Label: cooking (Confidence: 0.5931)\n",
      "Text: 'Beginner's guide to weightlifting at home'\n",
      "  Predicted Label: fitness (Confidence: 0.4150)\n",
      "Text: 'Breaking news from around the world'\n",
      "  Predicted Label: fitness (Confidence: 0.3716)\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "import joblib\n",
    "\n",
    "# Load the saved model and tokenizer\n",
    "model_path = \"./fine_tuned_distilbert_bookmark_classifier3\"\n",
    "loaded_tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "loaded_model = AutoModelForSequenceClassification.from_pretrained(model_path)\n",
    "loaded_model.to(device) # Ensure it's on the correct device\n",
    "\n",
    "# Load the label encoder\n",
    "loaded_label_encoder = joblib.load(f\"{model_path}/label_encoder.pkl\")\n",
    "\n",
    "# Create a Hugging Face pipeline for easy inference\n",
    "classifier = pipeline(\n",
    "    \"text-classification\",\n",
    "    model=loaded_model,\n",
    "    tokenizer=loaded_tokenizer,\n",
    "    device=0 if torch.cuda.is_available() else -1 # 0 for GPU, -1 for CPU\n",
    ")\n",
    "\n",
    "# Example predictions\n",
    "new_texts = [\n",
    "    \"Delicious recipe for carbonara pasta\",\n",
    "    \"Best tips for learning Python in 2024\",\n",
    "    \"Stylish autumn outfits for women\",\n",
    "    \"Beginner's guide to weightlifting at home\",\n",
    "    \"Breaking news from around the world\" # This might be misclassified if not in training\n",
    "]\n",
    "\n",
    "print(\"\\n--- Making predictions on new text ---\")\n",
    "for text in new_texts:\n",
    "    prediction = classifier(text)[0]\n",
    "    predicted_label_id = prediction['label'] # This will be the numerical ID string, e.g., \"LABEL_0\"\n",
    "    score = prediction['score']\n",
    "\n",
    "    # Convert back to original label using the label encoder\n",
    "    # Note: HuggingFace's `pipeline` often returns labels like \"LABEL_0\", \"LABEL_1\" if not\n",
    "    # explicitly provided `id2label` during pipeline creation.\n",
    "    # We can also get the ID from the `id2label` attribute of the model if correctly set.\n",
    "    # Let's use the loaded_label_encoder for robustness.\n",
    "    # Assuming 'LABEL_X' where X is the numerical ID.\n",
    "    try:\n",
    "        numerical_id = int(predicted_label_id.replace(\"LABEL_\", \"\"))\n",
    "        predicted_original_label = loaded_label_encoder.inverse_transform([numerical_id])[0]\n",
    "    except ValueError:\n",
    "        # Fallback if label is directly the string (e.g., 'cooking') if pipeline is smarter\n",
    "        predicted_original_label = predicted_label_id\n",
    "\n",
    "\n",
    "    print(f\"Text: '{text}'\")\n",
    "    print(f\"  Predicted Label: {predicted_original_label} (Confidence: {score:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b63aaaf-ab8d-4635-bb1c-a4d6a6b28b80",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
